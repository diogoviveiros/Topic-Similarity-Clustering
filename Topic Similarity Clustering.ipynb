{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1aec641-dd10-4ce5-870c-0ea88792da67",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import transformers\n",
    "from flask import Flask, flash, request, redirect, url_for, render_template \n",
    "from werkzeug.utils import secure_filename\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ab3528b-44c1-481f-845d-36deb752d492",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Download corpus of common stopwords\n",
    "nltk.download('stopwords')\n",
    "stops = set(stopwords.words(\"english\"))\n",
    "\n",
    "\n",
    "#Remove stopwords from text to create higher differences between text by removing common words that don't matter for our context\n",
    "def remove_stopwords(sentence):\n",
    "    words = str(sentence).split()\n",
    "    filtered_sentence = [word for word in words if word.lower() not in stops]\n",
    "    return ' '.join(filtered_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d327fc64-433f-4b96-9eb7-ed1ed53d73a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "app = Flask(__name__)\n",
    "\n",
    "\n",
    "#Set 500 MB filesize limit\n",
    "app.config['MAX_CONTENT_LENGTH'] = 500 * 1000 * 1000\n",
    "UPLOAD_FOLDER = '/path/to/the/uploads'\n",
    "ALLOWED_EXTENSIONS = {'csv'}\n",
    "\n",
    "\n",
    "def allowed_file(filename):\n",
    "    return '.' in filename and \\\n",
    "           filename.rsplit('.', 1)[1].lower() in ALLOWED_EXTENSIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1e41fe8-25db-439d-8264-04d22182bacd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def read_file(file):\n",
    "    # Load CSV file into a DataFrame\n",
    "    topics = pd.read_csv(file)\n",
    "\n",
    "    return topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ceb0883-7d2d-4c48-b7b5-93aa4d9ff8ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data(topics):\n",
    "\n",
    "    df = topics.copy()\n",
    "\n",
    "    #Clean up Description columns, remove all the HTML jargon, along with any \"&nbsp; ,&amp &gt, &amp;\n",
    "    patterns = '|'.join([r'<[^<>]*>', r'&nbsp;', r'&gt;', r'&amp'])\n",
    "\n",
    "    df['Description'] = df['Description'].str.replace(patterns, ' ', regex=True)\n",
    "\n",
    "    #Remove stopwords using the function we set up above\n",
    "    df['Description'] = df['Description'].apply(remove_stopwords)\n",
    "\n",
    "    #Check if the description looks okay\n",
    "    print(df['Description'][0])\n",
    "\n",
    "    #Merge Title and Description columns since we're going to be running context analysis on them combined. \n",
    "    df['Description'] = df['Title'] + \" \" + \"\\n\" + df['Description']\n",
    "\n",
    "    #Remove Title since it is now merged with description\n",
    "    df = df.drop(['Title'], axis=1)\n",
    "\n",
    "    print(df['Description'])\n",
    "\n",
    "    #Start prepping to feed text into model\n",
    "    #Remove \\ns from text, might influence results\n",
    "\n",
    "    df['Description'] = df['Description'].str.replace('\\n' , '')\n",
    "\n",
    "    # Convert DataFrame to NumPy array so that model can interpret it\n",
    "    data = df['Description'].to_numpy()\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2edcac6-914c-4bb3-bb53-cfc0283a9d59",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_model(data):\n",
    "    \n",
    "    #Load Pretrained Model\n",
    "    model = SentenceTransformer('sentence-transformers/all-mpnet-base-v2')\n",
    "\n",
    "    #Calculate embeddings for cleaned data\n",
    "    embeddings = model.encode(data)\n",
    "    embeddings.shape\n",
    "\n",
    "    return embeddings\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dcc9b74-0397-447e-b4c0-e7d4dd5c21ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_similarities(embeddings):\n",
    "    \n",
    "    #Calculating embeddings similarities\n",
    "    similarities = model.similarity(embeddings, embeddings)\n",
    "    print(similarities)\n",
    "\n",
    "    #Convert the pytorch tensor to a numpy array\n",
    "\n",
    "    matrix = similarities.numpy()\n",
    "\n",
    "    similarity_matrix = pd.DataFrame(similarities)\n",
    "\n",
    "    return similarity_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "191b7b75-415b-4ea0-b3ed-0913f4b0071e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_cluster(topics, embeddings, num_clusters, cosine_sensitivity):\n",
    "\n",
    "    #TODO: Play around with HDBSCAN\n",
    "\n",
    "    #TODO: Try hierarchichal Clustering\n",
    "\n",
    "    print(\"Num clusters is:\")\n",
    "    print(num_clusters)\n",
    "    print(\"Sensitivity is:\")\n",
    "    print (cosine_sensitivity)\n",
    "  \n",
    "\n",
    "    # Perform agglomerative clustering\n",
    "    clustering_model = AgglomerativeClustering(\n",
    "        n_clusters= num_clusters, distance_threshold= cosine_sensitivity,\n",
    "        metric = 'cosine', linkage='average'\n",
    "    )  # , affinity='cosine', linkage='average', distance_threshold=0.4)\n",
    "    clustering_model.fit(embeddings)\n",
    "    cluster_assignment = clustering_model.labels_\n",
    "    topics[\"n_cluster\"] = cluster_assignment\n",
    "\n",
    "    return topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26f60088-d4c6-4d68-8408-af4297e35d99",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_output(topics):\n",
    "    topics = topics.sort_values(by=['n_cluster'])\n",
    "    \n",
    "    topics.to_csv('cluster_output.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdc1f098-a6a0-47a3-8c14-80b086fcfc9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "@app.route('/', methods=['GET', 'POST'])\n",
    "def index_page():\n",
    "    if request.method == 'POST':\n",
    "        # check if the post request has the file part\n",
    "        if 'file' not in request.files:\n",
    "            flash('No file part')\n",
    "            return redirect(request.url)\n",
    "        file = request.files['file']\n",
    "        clustering_type = request.form.get(\"clusteringType\")\n",
    "        num_clusters = None\n",
    "        cosine_sensitivity = None               \n",
    "        if(clustering_type == \"auto\"):\n",
    "            print(\"Entering if condition\")\n",
    "            cosine_sensitivity = float(request.form.get(\"sensitivity\"))\n",
    "            print(\"sensitivity is:\")\n",
    "            print(cosine_sensitivity)\n",
    "        else:\n",
    "            print(\"Entering else condition\")\n",
    "            num_clusters = int(request.form.get(\"numClusters\"))\n",
    "        # If the user does not select a file, the browser submits an\n",
    "        # empty file without a filename.\n",
    "        if file.filename == '':\n",
    "            flash('No selected file')\n",
    "            return redirect(request.url)\n",
    "        if file and allowed_file(file.filename):\n",
    "            filename = secure_filename(file.filename)\n",
    "            #Run things here\n",
    "            topics = read_file(file)\n",
    "            data = prepare_data(topics)\n",
    "            embeddings = run_model(data)\n",
    "            #similarities = calculate_similarities(embeddings)\n",
    "            topics = generate_cluster(topics, embeddings, num_clusters, cosine_sensitivity)\n",
    "            save_output(topics)            \n",
    "            print('Done')\n",
    "            \n",
    "            return  ''' <!doctype html>\n",
    "    <title>Upload new File</title>\n",
    "    <h1>Upload new File</h1>\n",
    "    <form method=post enctype=multipart/form-data>\n",
    "      <input type=file name=file accept = .csv>\n",
    "      <input type=submit value=Upload>\n",
    "    </form>\n",
    "    '''\n",
    "            \n",
    "    return render_template(\"Clustering Page.html\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "247291db-6fcc-4f68-8c91-2ddb6ea09f9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    app.run()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
